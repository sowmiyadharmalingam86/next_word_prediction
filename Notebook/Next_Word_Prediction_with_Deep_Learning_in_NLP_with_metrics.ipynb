{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 19:33:05.215390: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-18 19:33:05.250833: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-18 19:33:05.878059: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, SparseTopKCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 84699\n",
      "Validation samples: 10587\n",
      "Test samples: 10588\n",
      "Vocabulary size: 8922\n",
      "Max sequence length: 251\n"
     ]
    }
   ],
   "source": [
    "def file_to_sentence_list(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    sentences = [sentence.strip() for sentence in re.split(r'(?<=[.!?])\\s+', text) if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "file_path = '../data/raw/sherlock_holmes.txt'  # update to your dataset path\n",
    "text_data = file_to_sentence_list(file_path)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences\n",
    "input_sequences = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[: i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = np.array(\n",
    "    pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    ")\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# Split into train/validation/test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "y_val = y_val.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "\n",
    "print(f'Training samples: {X_train.shape[0]}')\n",
    "print(f'Validation samples: {X_val.shape[0]}')\n",
    "print(f'Test samples: {X_test.shape[0]}')\n",
    "print(f'Vocabulary size: {total_words}')\n",
    "print(f'Max sequence length: {max_sequence_len}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/practice/reiterate-2/next_word_prediction/.venv/lib/python3.13/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1760796206.851607  359237 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">571,008</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8922</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,150,938</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │       \u001b[38;5;34m571,008\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8922\u001b[0m)           │     \u001b[38;5;34m1,150,938\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,820,762</span> (6.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,820,762\u001b[0m (6.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,820,762</span> (6.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,820,762\u001b[0m (6.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len - 1))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.build((None, max_sequence_len - 1))\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=[\n",
    "        SparseCategoricalAccuracy(name='accuracy'),\n",
    "        SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy'),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 192ms/step - accuracy: 0.0546 - loss: 6.6431 - top_5_accuracy: 0.1582 - val_accuracy: 0.0660 - val_loss: 6.4415 - val_top_5_accuracy: 0.1777 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 192ms/step - accuracy: 0.0801 - loss: 6.0935 - top_5_accuracy: 0.2145 - val_accuracy: 0.0887 - val_loss: 6.1379 - val_top_5_accuracy: 0.2445 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 191ms/step - accuracy: 0.1088 - loss: 5.7192 - top_5_accuracy: 0.2708 - val_accuracy: 0.1185 - val_loss: 5.9466 - val_top_5_accuracy: 0.2808 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 196ms/step - accuracy: 0.1286 - loss: 5.4611 - top_5_accuracy: 0.2981 - val_accuracy: 0.1274 - val_loss: 5.8692 - val_top_5_accuracy: 0.2898 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 193ms/step - accuracy: 0.1418 - loss: 5.2650 - top_5_accuracy: 0.3156 - val_accuracy: 0.1308 - val_loss: 5.8320 - val_top_5_accuracy: 0.3002 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 197ms/step - accuracy: 0.1512 - loss: 5.0953 - top_5_accuracy: 0.3306 - val_accuracy: 0.1358 - val_loss: 5.8176 - val_top_5_accuracy: 0.3076 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 191ms/step - accuracy: 0.1598 - loss: 4.9439 - top_5_accuracy: 0.3450 - val_accuracy: 0.1387 - val_loss: 5.8198 - val_top_5_accuracy: 0.3126 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 197ms/step - accuracy: 0.1678 - loss: 4.8046 - top_5_accuracy: 0.3560 - val_accuracy: 0.1389 - val_loss: 5.8319 - val_top_5_accuracy: 0.3153 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.1723 - loss: 4.7128 - top_5_accuracy: 0.3631\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 193ms/step - accuracy: 0.1750 - loss: 4.6699 - top_5_accuracy: 0.3673 - val_accuracy: 0.1419 - val_loss: 5.8566 - val_top_5_accuracy: 0.3178 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 196ms/step - accuracy: 0.1842 - loss: 4.5130 - top_5_accuracy: 0.3800 - val_accuracy: 0.1432 - val_loss: 5.8721 - val_top_5_accuracy: 0.3184 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 198ms/step - accuracy: 0.1886 - loss: 4.4445 - top_5_accuracy: 0.3873 - val_accuracy: 0.1440 - val_loss: 5.8929 - val_top_5_accuracy: 0.3206 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.1895 - loss: 4.4233 - top_5_accuracy: 0.3872\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 191ms/step - accuracy: 0.1919 - loss: 4.3775 - top_5_accuracy: 0.3933 - val_accuracy: 0.1448 - val_loss: 5.9176 - val_top_5_accuracy: 0.3210 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 195ms/step - accuracy: 0.1973 - loss: 4.3022 - top_5_accuracy: 0.4018 - val_accuracy: 0.1457 - val_loss: 5.9287 - val_top_5_accuracy: 0.3221 - learning_rate: 2.5000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 196ms/step - accuracy: 0.1997 - loss: 4.2670 - top_5_accuracy: 0.4062 - val_accuracy: 0.1459 - val_loss: 5.9445 - val_top_5_accuracy: 0.3211 - learning_rate: 2.5000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.1990 - loss: 4.2799 - top_5_accuracy: 0.4027\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 192ms/step - accuracy: 0.2020 - loss: 4.2318 - top_5_accuracy: 0.4096 - val_accuracy: 0.1459 - val_loss: 5.9630 - val_top_5_accuracy: 0.3218 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 198ms/step - accuracy: 0.2045 - loss: 4.1982 - top_5_accuracy: 0.4139 - val_accuracy: 0.1470 - val_loss: 5.9702 - val_top_5_accuracy: 0.3214 - learning_rate: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "EPOCHS = 100  # adjust based on dataset size and compute budget\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "  loss: 4.9235\n",
      "  accuracy: 0.1611\n",
      "  top_5_accuracy: 0.3450\n",
      "  perplexity: 137.4884\n",
      "\n",
      "Validation metrics:\n",
      "  loss: 5.8176\n",
      "  accuracy: 0.1358\n",
      "  top_5_accuracy: 0.3076\n",
      "  perplexity: 336.1527\n",
      "\n",
      "Test metrics:\n",
      "  loss: 5.8391\n",
      "  accuracy: 0.1357\n",
      "  top_5_accuracy: 0.3087\n",
      "  perplexity: 343.4813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics and perplexity\n",
    "import math\n",
    "\n",
    "def evaluate_split(features, labels):\n",
    "    loss, accuracy, top5 = model.evaluate(features, labels, verbose=0)\n",
    "    return {\n",
    "        'loss': float(loss),\n",
    "        'accuracy': float(accuracy),\n",
    "        'top_5_accuracy': float(top5),\n",
    "        'perplexity': float(math.exp(loss)),\n",
    "    }\n",
    "\n",
    "metrics_summary = {\n",
    "    'train': evaluate_split(X_train, y_train),\n",
    "    'validation': evaluate_split(X_val, y_val),\n",
    "    'test': evaluate_split(X_test, y_test),\n",
    "}\n",
    "\n",
    "for split, stats in metrics_summary.items():\n",
    "    print(f\"{split.capitalize()} metrics:\")\n",
    "    for metric_name, value in stats.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to Notebook/artifacts/notebook_model.keras\n",
      "Saved tokenizer config to Notebook/artifacts/tokenizer.json\n",
      "Saved metadata to Notebook/artifacts/notebook_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Persist model and tokenizer for later inference\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "artifacts_dir = Path(\"Notebook\") / \"artifacts\"\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = artifacts_dir / \"notebook_model.keras\"\n",
    "tokenizer_path = artifacts_dir / \"tokenizer.json\"\n",
    "metadata_path = artifacts_dir / \"notebook_metadata.json\"\n",
    "\n",
    "model.save(model_path, include_optimizer=False)\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "tokenizer_path.write_text(tokenizer_json, encoding=\"utf-8\")\n",
    "\n",
    "metadata = {\n",
    "    \"max_sequence_len\": int(max_sequence_len),\n",
    "    \"vocab_size\": int(total_words),\n",
    "    \"train_samples\": int(X_train.shape[0]),\n",
    "    \"val_samples\": int(X_val.shape[0]),\n",
    "    \"test_samples\": int(X_test.shape[0]),\n",
    "}\n",
    "metadata_path.write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved model to {model_path}\")\n",
    "print(f\"Saved tokenizer config to {tokenizer_path}\")\n",
    "print(f\"Saved metadata to {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation helper with top-k probabilities\n",
    "def generate_text(seed_text, num_words=30, top_k=5, temperature=1.0):\n",
    "    generated = seed_text.strip()\n",
    "    step_details = []\n",
    "\n",
    "    for step in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([generated])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predictions = model.predict(token_list, verbose=0)[0]\n",
    "        predictions = predictions.astype('float64')\n",
    "\n",
    "        if temperature != 1.0 and temperature > 0:\n",
    "            predictions = np.log(predictions + 1e-8) / temperature\n",
    "            predictions = np.exp(predictions)\n",
    "            predictions = predictions / np.sum(predictions)\n",
    "\n",
    "        top_indices = predictions.argsort()[-top_k:][::-1]\n",
    "        top_predictions = [\n",
    "            (tokenizer.index_word.get(idx, ''), float(predictions[idx]))\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "\n",
    "        next_index = top_indices[0]\n",
    "        next_word = tokenizer.index_word.get(next_index, '')\n",
    "        generated = f\"{generated} {next_word}\".strip()\n",
    "\n",
    "        step_details.append(\n",
    "            {\n",
    "                'step': step + 1,\n",
    "                'next_word': next_word,\n",
    "                'top_k': top_predictions,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return generated, step_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: \"I saw Holmes\"\n",
      "Generated text (length 30 words):\n",
      "I saw Holmes in the door and i had been a very man of the matter ” said holmes was a very man of the door of the door of the door of\n",
      "\n",
      "Top-5 predictions at each generation step:\n",
      "Step  1: next word = in | top-5 -> in (0.047), and (0.047), to (0.036), ” (0.030), as (0.023)\n",
      "Step  2: next word = the | top-5 -> the (0.367), a (0.099), my (0.061), his (0.051), this (0.025)\n",
      "Step  3: next word = door | top-5 -> door (0.015), room (0.012), very (0.011), matter (0.009), house (0.009)\n",
      "Step  4: next word = and | top-5 -> and (0.218), of (0.149), which (0.053), ” (0.050), in (0.047)\n",
      "Step  5: next word = i | top-5 -> i (0.098), he (0.042), the (0.039), a (0.035), we (0.025)\n",
      "Step  6: next word = had | top-5 -> had (0.105), have (0.073), was (0.052), am (0.034), could (0.030)\n",
      "Step  7: next word = been | top-5 -> been (0.147), not (0.039), a (0.029), no (0.024), seen (0.023)\n",
      "Step  8: next word = a | top-5 -> a (0.041), in (0.033), to (0.024), the (0.015), been (0.014)\n",
      "Step  9: next word = very | top-5 -> very (0.040), little (0.039), man (0.019), few (0.014), good (0.014)\n",
      "Step 10: next word = man | top-5 -> man (0.015), little (0.013), very (0.011), good (0.008), few (0.008)\n",
      "Step 11: next word = of | top-5 -> of (0.213), ” (0.101), and (0.060), to (0.046), in (0.041)\n",
      "Step 12: next word = the | top-5 -> the (0.144), my (0.046), this (0.043), a (0.040), which (0.029)\n",
      "Step 13: next word = matter | top-5 -> matter (0.012), man (0.012), very (0.011), room (0.010), door (0.010)\n",
      "Step 14: next word = ” | top-5 -> ” (0.195), of (0.102), and (0.092), to (0.053), in (0.040)\n",
      "Step 15: next word = said | top-5 -> said (0.093), “i (0.069), he (0.051), “and (0.050), “yes (0.037)\n",
      "Step 16: next word = holmes | top-5 -> holmes (0.341), he (0.177), i (0.096), that (0.037), the (0.033)\n",
      "Step 17: next word = was | top-5 -> was (0.047), ” (0.033), i (0.028), as (0.025), is (0.024)\n",
      "Step 18: next word = a | top-5 -> a (0.210), the (0.072), in (0.034), no (0.020), very (0.014)\n",
      "Step 19: next word = very | top-5 -> very (0.018), little (0.016), man (0.015), few (0.013), small (0.008)\n",
      "Step 20: next word = man | top-5 -> man (0.015), few (0.007), little (0.007), very (0.006), public (0.006)\n",
      "Step 21: next word = of | top-5 -> of (0.214), and (0.108), ” (0.059), in (0.036), which (0.031)\n",
      "Step 22: next word = the | top-5 -> the (0.142), a (0.041), which (0.036), his (0.031), this (0.024)\n",
      "Step 23: next word = door | top-5 -> door (0.013), room (0.010), man (0.009), very (0.009), house (0.008)\n",
      "Step 24: next word = of | top-5 -> of (0.193), and (0.177), ” (0.059), in (0.053), which (0.051)\n",
      "Step 25: next word = the | top-5 -> the (0.224), a (0.068), his (0.047), my (0.027), this (0.025)\n",
      "Step 26: next word = door | top-5 -> door (0.014), room (0.010), man (0.008), very (0.008), house (0.008)\n",
      "Step 27: next word = of | top-5 -> of (0.204), and (0.171), which (0.054), in (0.049), ” (0.045)\n",
      "Step 28: next word = the | top-5 -> the (0.222), a (0.062), his (0.048), this (0.023), my (0.021)\n",
      "Step 29: next word = door | top-5 -> door (0.014), room (0.009), man (0.008), very (0.008), house (0.007)\n",
      "Step 30: next word = of | top-5 -> of (0.204), and (0.163), which (0.057), in (0.046), ” (0.034)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: \"The adventure\"\n",
      "Generated text (length 30 words):\n",
      "The adventure of the door and i had been a very man of the matter ” said holmes was a very man of the door of the door of the door of\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Seed: \"Dr Watson\"\n",
      "Generated text (length 30 words):\n",
      "Dr Watson ” said holmes was a little man and i had been a very man of the matter ” said holmes was a very man of the door of the door\n"
     ]
    }
   ],
   "source": [
    "# Generate sample outputs with detailed top-5 breakdown\n",
    "examples = [\n",
    "    (\"I saw Holmes\", 30),\n",
    "    (\"The adventure\", 30),\n",
    "    (\"Dr Watson\", 30),\n",
    "]\n",
    "\n",
    "primary_seed, primary_len = examples[0]\n",
    "primary_text, primary_steps = generate_text(primary_seed, num_words=primary_len)\n",
    "\n",
    "print(f'Seed: \"{primary_seed}\"')\n",
    "print(f'Generated text (length {primary_len} words):\\n{primary_text}\\n')\n",
    "print('Top-5 predictions at each generation step:')\n",
    "for info in primary_steps:\n",
    "    topk_formatted = ', '.join([f\"{word} ({prob:.3f})\" for word, prob in info['top_k']])\n",
    "    print(f\"Step {info['step']:>2}: next word = {info['next_word']} | top-5 -> {topk_formatted}\")\n",
    "\n",
    "for seed, length in examples[1:]:\n",
    "    text, _ = generate_text(seed, num_words=length)\n",
    "    print('\\n' + '-' * 80)\n",
    "    print(f'Seed: \"{seed}\"')\n",
    "    print(f'Generated text (length {length} words):\\n{text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Next Word Prediction",
   "language": "python",
   "name": "next-word-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
